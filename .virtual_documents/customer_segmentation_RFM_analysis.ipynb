%matplotlib inline


import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

import re
from collections import defaultdict
from functools import reduce

from sklearn.ensemble import IsolationForest

from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

from sklearn.metrics import silhouette_score

from sklearn.cluster import KMeans
from collections import Counter

from yellowbrick.cluster import SilhouetteVisualizer

from mpl_toolkits.mplot3d import Axes3D


# pip install yellowbrick




















# xls_data = pd.read_excel("data/Online_Retail.xlsx")


# xls_data.to_csv("data/online_retail_data.csv", encoding = "ISO-8859-1", index = False)


data = pd.read_csv("data/online_retail_data.csv", encoding = "ISO-8859-1")


data.columns


data.info()


data.shape


data.describe().T


data.describe(include = ["object"]).T


data.isnull().sum()











data = data.rename(columns = {'InvoiceNo': 'invoice_no',
                       'StockCode': 'stockcode',
                       'Description': 'description', 
                       'Quantity': 'quantity', 
                       'InvoiceDate': 'invoice_date',
                       'UnitPrice': 'unit_price',
                       'CustomerID': 'customer_id', 
                       'Country': 'country',
                             })





pct_missing_descr = round(len(data[data.description.isnull()]) / len(data) * 100, 2)
pct_missing_id = round(len(data[data.customer_id.isnull()]) / len(data) * 100, 2)


sizes_descr = [pct_missing_descr, 100 - pct_missing_descr]
sizes_id = [pct_missing_id, 100 - pct_missing_id]
labels = ["missing values", ""]

fig, axes = plt.subplots(1, 2, figsize = (10, 4))
axes[0].pie(sizes_descr, labels = labels, autopct = "%1.2f%%", startangle = 140, pctdistance = 1.3, wedgeprops = {'edgecolor': 'black'})
axes[0].set_title("Description column")

axes[1].pie(sizes_id, labels = labels, autopct = "%1.2f%%", startangle = 90, pctdistance = 1.3, wedgeprops = {'edgecolor': 'black'})
axes[1].set_title("Customer ID column")

plt.show()


data = data.dropna(subset = ["description", "customer_id"])


data.shape





data[data.duplicated()]


data = data.drop_duplicates(ignore_index = True)


data[data.duplicated()]


data.invoice_no.value_counts()


c_invoices = data[data.invoice_no.str.startswith("C")]
c_invoices


all(c_invoices[c_invoices.quantity < 0])





data["tr_status"] = data.quantity.apply(lambda x: 0 if x < 0 else 1)


data.sample(5)


cancelled_trs_count = len(data[data.tr_status == 0])
print(f"{(cancelled_trs_count / len(data)) * 100:.2f}% or {cancelled_trs_count} transactions are cancelled.")


data.stockcode.value_counts()


unique_codes = data.stockcode.unique()


print(f"There are {len(unique_codes)} unique products (stockcodes).")





stockcode_with_chars = pd.DataFrame([code for code in unique_codes if not(code.isnumeric())])
print(f"There are {len(stockcode_with_chars)} unique stockcodes containing character/s.")


stockcode_chars_nums = pd.Series(["".join(el for el in code if el.isalpha()) for code in unique_codes if not(code.isnumeric())])
stockcode_chars_nums.value_counts()





char_to_stockcodes = defaultdict(set)

for code in data.stockcode:
    for char in set(code): # ensures each char is counted once per stockcode (avoids duplicate letters)
        if char.isalpha(): 
            char_to_stockcodes[char].add(code)


char_stock_df = pd.DataFrame({"character": list(char_to_stockcodes.keys()), 
                              "codes": [", ".join(sorted(stockcodes)) for stockcodes in char_to_stockcodes.values()],
                             })

char_stock_df["counts_codes_containing_char"] = char_stock_df.codes.apply(lambda x: len(x.split(", ")))
char_stock_df





filtered_stockcodes = data[data.stockcode.str.count(r'[A-Za-z]') >= 3]
filtered_stockcodes.stockcode.value_counts()





small_stockcodes = data[data.stockcode.str.len() <= 3]
small_stockcodes.stockcode.value_counts()





anomalies = ["POST", "BANK CHARGES", "PADS", "DOT", "CRUK", "M", "C2", "D"]


anomalies_stockcodes = data[data.stockcode.isin(anomalies)]
len(anomalies_stockcodes)


anomalies_stockcodes.description.unique()





data = data[~data.stockcode.isin(anomalies)]


data.shape





data.description.unique()


len(data.description.unique())





data.description = data.description.str.strip()


len(data.description.unique())





lowercase_descr = [d for d in data.description.unique() if any(char.islower() for char in d)]
lowercase_descr





len(data[data.description.isin(['Next Day Carriage', 'High Resolution Image'])])


data = data[~data.description.isin(['Next Day Carriage', 'High Resolution Image'])]





data.description = data.description.str.upper()


top_descriptions = data.description.value_counts()[:50]

plt.figure(figsize = (8, 10))
sns.barplot(
    y = top_descriptions.index,
    x = top_descriptions.values,
    palette = "Reds_r",  
    hue = top_descriptions.index, 
    edgecolor = "black")

plt.xlabel("count")
plt.ylabel("StockCode")
plt.title(f"Top 50 most frequent descriptions")
plt.grid(axis = 'x', linestyle = '--', alpha = 0.7)
plt.show()





data.quantity.value_counts()


plt.figure(figsize = (4, 6))

plt.boxplot(data.quantity)
plt.ylabel("values")
plt.title("Quantity")
plt.show()





data.invoice_date





data.invoice_date = pd.to_datetime(data.invoice_date)


data.info()





plt.figure(figsize = (4, 6))

plt.boxplot(data.unit_price)
plt.ylabel("price values")
plt.title("Price")
plt.show()


max_price_records = data[data.unit_price == data.unit_price.max()]
max_price_records


min_price_records = data[data.unit_price == data.unit_price.min()]
min_price_records


print(f"There are {len(min_price_records)} products with 0 price.")





data = data[data.unit_price > 0]





print(f"There are {len(data.customer_id.unique())} customers in this dataset.")


top_customers = data.customer_id.value_counts()
top_customers





unique_products_by_customer = data.groupby("customer_id")["stockcode"].nunique().reset_index()
unique_products_by_customer.rename(columns = {"stockcode": "count_unique_products"}, inplace = True)
unique_products_by_customer








print(f"There are {len(data.country.unique())} unique countries in this dataset.")


ordered_countries = pd.DataFrame(data.country.value_counts()).reset_index()

plt.figure(figsize = (12, 4))
plt.bar(ordered_countries.country, ordered_countries["count"])
plt.xticks(rotation = 80)
plt.show()


top_country_pct = len(data[data.country == "United Kingdom"]) / len(data) * 100
print(f"{top_country_pct:.2f} of purchases are from United Kingdom.")








# get the most recent date in the dataset:
reference_date = data.invoice_date.max()

# To compute Recency I group by 'customer_id' and find the most recent purchase date for each customer.
# Then, I subtract it from the reference_date to get the number of days since the last purchase.
recency = data.groupby("customer_id")["invoice_date"].max().reset_index()
recency["recency"] = (reference_date - recency["invoice_date"]).dt.days  # days since last purchase
recency.drop(columns = ["invoice_date"], inplace = True)

# recency shows the days since last purchase:
recency.head()





# to compute frequency I group by 'customer_id' and count the number of unique 'invoice' entries for each customer:
frequency = data.groupby("customer_id")["invoice_no"].nunique().reset_index()
frequency.rename(columns = {"invoice_no": "frequency"}, inplace = True)
frequency.head()





# compute monetary: total amount of spent transactions = quantity * unit_price:
data["total_spent"] = data["quantity"] * data["unit_price"]

# group by 'customer_id' and sum 'total_spent' to get the total amount spent per customer:
monetary = data.groupby("customer_id")["total_spent"].sum().reset_index()
monetary.rename(columns = {"total_spent": "monetary"}, inplace = True) 
monetary.head()





# find total spend per customer
total_spend = data.groupby("customer_id", as_index = False).agg({"total_spent": "sum"})

# compute total transactions per customer
total_transactions = data.groupby("customer_id", as_index = False).agg({"invoice_no": "nunique"})
total_transactions.rename(columns={"invoice_no": "total_transactions"}, inplace = True)

# compute ATV:
average_transaction_value = total_spend.merge(total_transactions, on = "customer_id")
average_transaction_value["average_transaction_value"] = (
    average_transaction_value["total_spent"] / average_transaction_value["total_transactions"])

ATV_data = total_spend.merge(average_transaction_value[["customer_id", "average_transaction_value"]], on = "customer_id")
ATV_data





customer_location = data[["customer_id", "country"]].drop_duplicates()
customer_location["is_from_uk"] = customer_location["country"].apply(lambda x: 1 if x == "United Kingdom" else 0)
customer_location = customer_location.drop(columns = ["country"])
customer_location.is_from_uk.value_counts()





data.invoice_date.dt.month.value_counts()


# extract the day and the hour of the week:
data["shopping_day"] = data["invoice_date"].dt.dayofweek

# compute the most frequent shopping day per customer using mode:
top_shopping_day = data.groupby("customer_id")["shopping_day"].agg(lambda x: x.mode()[0]).reset_index()
top_shopping_day.rename(columns = {"shopping_day": "favorite_shopping_day"}, inplace=True)





# extract year-month:
data["year_month"] = data["invoice_date"].dt.to_period("M")

# compute total monthly spending per customer
monthly_spending = data.groupby(["customer_id", "year_month"])["total_spent"].sum().reset_index()

# Step 4: Compute the Monthly Spending Mean per Customer
monthly_spending_mean = monthly_spending.groupby("customer_id")["total_spent"].mean().reset_index()
monthly_spending_mean.rename(columns = {"total_spent": "monthly_spending_mean"}, inplace = True)
monthly_spending_mean





# Identify cancelled transactions - if quantity < 0:
data["is_cancelled"] = data["quantity"].apply(lambda x: 1 if x < 0 else 0)

# compute cancellation frequency per customer - count the number of cancelled transactions / customer:
cancellation_frequency = data.groupby("customer_id")['is_cancelled'].sum().reset_index()
cancellation_frequency.rename(columns = {"is_cancelled": "cancellation_frequency"}, inplace = True)

# compute total transactions per customer - count the total number of transactions for each customer
total_transactions = data.groupby("customer_id")["invoice_no"].nunique().reset_index()
total_transactions.rename(columns = {"invoice_no": "total_transactions"}, inplace = True)

# cancellation rate = cancelled transactions / total transactions
cancellation_data = cancellation_frequency.merge(total_transactions, on = "customer_id")
cancellation_data["cancellation_rate"] = (cancellation_data["cancellation_frequency"] / cancellation_data["total_transactions"]).round(2)
cancellation_data = cancellation_data.drop(columns = ["total_transactions"])
cancellation_data





dfs = [unique_products_by_customer, 
       recency, 
       frequency, 
       monetary,
       ATV_data, 
       customer_location,
       top_shopping_day,
       monthly_spending_mean, 
       cancellation_data]


for i, df in enumerate(dfs):
    print(f"{i}: {df.columns}")


ATV_data = ATV_data[["customer_id", "average_transaction_value"]]


cleaned_dfs = [unique_products_by_customer, 
               recency, 
               frequency, 
               monetary,
               ATV_data, 
               customer_location,
               top_shopping_day,
               monthly_spending_mean, 
               cancellation_data]





extracted_customer_data = reduce(lambda left, right: pd.merge(left, right, on = "customer_id"), cleaned_dfs)


extracted_customer_data


extracted_customer_data.info()


extracted_customer_data.customer_id = extracted_customer_data.customer_id.astype(str)








features = extracted_customer_data.drop(columns = ["customer_id"])

# isolation forest with contamination ~ 5% anomalies
IF_model = IsolationForest(n_estimators = 100, contamination = 0.05, random_state = 42)
extracted_customer_data["anomaly_score"] = IF_model.fit_predict(features)

# label outliers with -1 = outlier, 1 = normal:
extracted_customer_data["is_outlier"] = extracted_customer_data["anomaly_score"].apply(lambda x: 1 if x == -1 else 0)


extracted_customer_data.is_outlier.value_counts()





clean_data = extracted_customer_data[extracted_customer_data.is_outlier == 0]

clean_data = clean_data.drop(columns = ["anomaly_score", "is_outlier"])
clean_data.reset_index(drop = True, inplace = True)


clean_data.shape


clean_data.head()





corr = clean_data.drop(columns = ["customer_id"]).corr()


corr


plt.figure(figsize = (8, 6))
sns.heatmap(corr, cmap = sns.cubehelix_palette(as_cmap = True), annot = True, fmt = ".2f", linewidths = 1)
plt.title("Correlation мatrix")
plt.show()








# select only columns that need scaling:
cols_to_scale = ['count_unique_products', 'recency', 'frequency', 'monetary', 'average_transaction_value',
            'monthly_spending_mean', 'cancellation_frequency', 'cancellation_rate']


scaler = StandardScaler()
clean_data[cols_to_scale] = scaler.fit_transform(clean_data[cols_to_scale])


clean_data.head()





clean_data.set_index("customer_id", inplace = True)


# apply PCA, keeping all components initially:
n = len(clean_data.columns)
pca = PCA(n_components = n)  
principal_components = pca.fit_transform(clean_data)


explained_variance = pca.explained_variance_ratio_
cumulative_variance = np.cumsum(explained_variance)

threshold = 0.90
opt_pc_count = np.argmax(cumulative_variance >= threshold) + 1 


fig, ax1 = plt.subplots(figsize = (8, 6))
bars = ax1.bar(range(1, len(explained_variance) + 1),
               explained_variance, 
               alpha = 0.6, 
               align = "center", 
               label = "Individual explained variance", 
               color = "royalblue")

ax2 = ax1.twinx()
ax2.plot(range(1, len(cumulative_variance) + 1),
         cumulative_variance,
         linewidth = 2, 
         marker = "o", 
         label = "Cumulative explained variance",
         color = "green")

ax1.axvline(x = optimal_pc_count, color = "red", linestyle = "--", alpha = 0.8, label = f"Optimal number of PCs = {opt_pc_count}")

for bar, var in zip(bars, explained_variance):
    height = bar.get_height()
    ax1.text(bar.get_x() + bar.get_width() / 2, height + 0.005, f"{var:.2f}", ha = "center")

ax1.set_xlabel("Number of principal components")
ax1.set_ylabel("Individual explained variance")
ax2.set_ylabel("Cumulative explained variance")
ax1.set_title("Explained variance per principal component")
ax2.grid(axis = "y", alpha = 0.6)
plt.xticks(range(1, len(cumulative_variance) + 1))
fig.legend(loc = "upper right", bbox_to_anchor = (0.9, 0.76))
plt.show()





pca = PCA(n_components = opt_pc_count)
pca_customer_data = pca.fit_transform(clean_data)
pca_customer_df = pd.DataFrame(pca_customer_data, columns = ["PC" + str(i+1) for i in range(pca.n_components_)])

# adding the customer_id index back to the new PCA df:
pca_customer_df.index = clean_data.index
pca_customer_df.head()


actual_features_used = clean_data.columns.tolist()

loadings_df = pd.DataFrame(
    pca.components_.T,
    index = actual_features_used,
    columns = [f'PC{i+1}' for i in range(pca.n_components_)]
)

loadings_df














k_range = range(2, 11)
elbow_scores, silhouette_scores = [], []

for k in k_range:
    km = KMeans(n_clusters = k, random_state = 42, n_init = 10)
    labels = km.fit_predict(pca_customer_df)
    elbow_scores.append(km.inertia_)
    silhouette_scores.append(silhouette_score(pca_customer_df, labels))

fig, ax1 = plt.subplots(figsize = (10, 6))

ax1.set_xlabel('Number of clusters (k)')
ax1.set_ylabel('Inertia (Elbow method)')
ax1.plot(k_range, elbow_scores, color = "royalblue", marker = "o", label = 'Inertia (Elbow method)')

ax2 = ax1.twinx()
ax2.set_ylabel('Silhouette Score')
ax2.plot(k_range, silhouette_scores, color = "red", marker = "s", label = 'Silhouette Score')

plt.title('Elbow and Silhouette scores')
fig.tight_layout()
plt.grid(axis = 'y')
plt.xticks(k_range)
plt.show()





k_range = range(2, 10) 

for k in k_range:
    kmeans = KMeans(n_clusters = k, n_init = 10, random_state = 42)
    visualizer = SilhouetteVisualizer(kmeans, colors = 'Set2', size = (600, 300))
    
    visualizer.fit(pca_customer_df) 
    visualizer.show()  





# run K-Means with optimal K = 3:
k_means = KMeans(n_clusters = 3, max_iter = 100, n_init = 10, random_state = 42)
k_means.fit(pca_customer_df)

original_labels = k_means.labels_


# frequency of each cluster
cluster_frequencies = Counter(original_labels)

# mapping clusters by frequency (largest cluster = 0, second largest = 1...)
sorted_clusters = [label for label, _ in cluster_frequencies.most_common()]
label_mapping = {old_label: new_label for new_label, old_label in enumerate(sorted_clusters)}

new_labels = np.array([label_mapping[label] for label in original_labels])
pca_customer_df["cluster"] = new_labels
clean_data["cluster"] = new_labels


clean_data.sample(5)


cluster_counts = pd.DataFrame(clean_data.cluster.value_counts()).reset_index()
cluster_counts


# calculate percentage of total records
cluster_counts["pct"] = (cluster_counts["count"] / cluster_counts["count"].sum()) * 100
cluster_counts


plt.figure(figsize = (8, 5))
barplot = sns.barplot(x = "cluster", y = "count", hue = "cluster", data = cluster_counts, palette = "Set1")

# add labels for counts and percentages on top of bars:
for index, row in cluster_counts.iterrows():
    barplot.text(index, row["count"] + 30, f'{row["count"]}\n({row["pct"]:.2f}%)', ha = "center")

plt.title("Cluster sizes")
plt.xlabel("cluster")
plt.ylabel("customers count")
plt.ylim(0, 2000)
plt.grid(axis = "y", alpha = 0.6)
plt.show()





x = pca_customer_df["PC1"]
y = pca_customer_df["PC2"]
z = pca_customer_df["PC3"]
clusters = pca_customer_df["cluster"]

fig = plt.figure(figsize = (12, 8))
palette = sns.color_palette("Set1", n_colors = 3)
ax = fig.add_subplot(111, projection = "3d")

# plot each cluster separately to match colors exactly:
for cluster_label in sorted(pca_customer_df.cluster.unique()):
    idx = clusters == cluster_label
    ax.scatter(
        x[idx], y[idx], z[idx], 
        s = 50, alpha = 0.7, 
        label = f"cluster {cluster_label}", 
        color = palette[cluster_label],
    )

ax.set_xlabel("PC 1")
ax.set_ylabel("PC 2")
ax.set_zlabel("PC 3")
ax.set_title("3D visualization of customer clusters", fontsize = 15)
ax.legend(title = "Legend:")
plt.show()








features_to_profile = [
    'count_unique_products', 'recency', 'frequency', 'monetary',
    'average_transaction_value', 'monthly_spending_mean',
    'cancellation_frequency', 'cancellation_rate'
]

cluster_profiles = clean_data.groupby('cluster')[features_to_profile].mean().round(2)

cluster_sizes = clean_data['cluster'].value_counts().sort_index()
cluster_profiles['num_customers'] = cluster_sizes.values
cluster_profiles








radar_data = cluster_profiles.drop(columns=["num_customers"])
features = radar_data.columns.tolist()
num_vars = len(features)

angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()
angles += angles[:1]

fig, ax = plt.subplots(figsize=(6, 6), subplot_kw=dict(polar=True))

for idx, row in radar_data.iterrows():
    values = row.tolist()
    values += values[:1] 
    ax.plot(angles, values, label=f"Cluster {idx}")
    ax.fill(angles, values, alpha=0.1)

ax.set_xticks(angles[:-1])
ax.set_xticklabels(features, fontsize=10)
ax.set_rlabel_position(30)
plt.yticks([-1, 0, 1, 2], ['-1', '0', '1', '2'], color='gray', size=8)
plt.ylim(-1.5, 2.5)
plt.title("Radar Chart for Customer Segments", size=14, pad=20)
plt.legend(loc="upper right", bbox_to_anchor=(1.1, 1.1))
plt.tight_layout()
plt.show()









